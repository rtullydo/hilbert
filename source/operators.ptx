<chapter xml:id="operators"><title>Linear operators</title>
    <section><title>Linear operators</title>
        <p>Linear operators are first encountered by students in calculus and differential equations, though not always with that name. Consider the problem
            <me>
                y^\prime = f(x)
            </me>
        which can be written
            <me>
                \frac{d}{dx} y = f.
            </me>
        More generally, we learn that any linear differential equation can be written in the form
        <me>
            L y = f
        </me>
        for the differential operator <m>L</m>. When we encounter problems like this the first time, we learn a group of recipes that solve the equation for various classes of <m>L</m>, functions <m>f</m> and sets of boundary conditions.</p>

        <p>In another set of classes, the linear algebra sequence, we learn an extensive set of tools for analyzing the matrix problem
            <me>
                A x = b
            </me>
        in terms of the structure of the matrix <m>A</m>. For example, we learn that the existence of a solution depends on the columnspace of <m>A</m> and that the structure of the solution set depends on the nullspace (or kernel, thinking of <m>A</m> as a function) of <m>A</m>.</p>

        <p>This chapter will examine the relationship between the matrix situation (in finite dimensions) and the linear operator situation (in infinite dimensions). A surprising amount of structure carries from finite to infinite dimensions.</p>

        <definition xml:id="def-linear-operator">
            <p>If <m>V, W</m> are vector spaces over a field <m>k</m>, a <term>linear operator</term> from <m>V</m> to <m>W</m> is a map <m>T:V \to W</m> such that
            <me>
                T(ax + by) = aT(x) + bT(y),
            </me>
            for all <m>a, b \in k</m> and <m>x, y \in V</m>. 
            </p>
        </definition>
        <p>A linear operator on <m>V</m> is a linear operator from <m>V</m> to <m>V</m>. (Note: these notes largely follow the same conventions as Axler's Linear Algebra Done Right. There is a difference here - Axler restricts the term operator to only the space <m>\mathcal{L}(V)</m> - that is, maps from <m>V \to V</m>.)</p>

        <p>If <m>V, W</m> are normed spaces, we say that a linear operator <m>T:V \to W</m> is <term>bounded</term> if there exists a constant <m>M \geq 0</m> such that
        <me>
            \norm{Tx} \leq M \norm{x}
        </me>
        for all <m>x \in V</m>. For a bounded <m>T</m>, we define the <term>operator norm</term> (or just norm when there is no ambiguity) of <m>T</m> to be the non-negative number
        <me>
            \norm{T} = \sup{\norm{Tx}:x\in V, \norm{x}\leq 1}.
        </me></p>

        <p>The <term>kernel</term> of a linear operator <m>T:V\to W</m> is the subspace of <m>V</m> given by <m>\{x: Tx = 0\}</m>. The kernel of <m>T</m> is denoted <m>\ker T</m>. The <term>range</term> of <m>T</m> is the subspace of <m>W</m> given by <m>\{Tx: x \in V\}</m>. The range of <m>T</m> is denoted <m>\ran T</m>.</p>

        <p><m>\norm{T}</m> can be thought of as the largest factor by which <m>T</m> stretches any vector, though it is a supremum, not a maximum. (What does this notion mean for a square matrix?) It also the case that
        <me>
            \norm{Tx} \leq \norm{T}\norm{x} \text{ (why?)}
        </me> </p>

        <example><title>the zero operator</title>
            <p>Let <m>V, W</m> be normed spaces. The map that sends every element of <m>V</m> to the zero element in <m>W</m> is a bounded operator of norm <m>0</m>.</p>
        </example>

        <example><title>a multiplication operator</title>
            <p>Choose a function <m>f \in C[a, b]</m> and define the operator <m>M_f</m> on <m>L^2[a,b]</m> by <me>(M_f x)(t) = f(t)x(t).</me> It should be immediately clear that <m>M_f</m> is linear. Recalling the norms on <m>C[a,b]</m> and <m>L^2</m>, for any <m>x\in L^2[a,b]</m> we get
            <md>
                <mrow> \norm{Mx}^2 \amp= \int_a^b \abs{f}^2 \abs{x}^2 \, dt </mrow>
                <mrow> \amp\leq \sup_{t \in [a, b]}\abs{f(t)}^2 \int_a^b \abs{x}^2 \, dt</mrow>
                <mrow>\amp = \norm{f}_\infty^2 \norm{x}^2.</mrow>
            </md>
            Then <m>M_f</m> is a bounded operator and <m>\norm{M} \leq \norm{f}_\infty</m>. It turns out to be the case that (usefully) <m>\norm{M} = \norm{f}_\infty</m>. (Can you prove it?)</p>
        </example>

        <example><title>an integral operator</title>
            <p> For real numbers <m>a, b, c, d</m>, let <m>k:[a,b]\times [c,d] \to \C</m> be a continuous map, and define <m>K:L^2[a,b] \to L^2[c,d]</m> by
            <me>
                (Kx)(t) = \int_a^b k(t, s) x(s) \, ds
            </me>
            for <m>t \in [c,d]</m>. <m>K</m> is linear (as integration is). For any <m>t\in [c,d]</m>, the Cauchy-Schwarz inequality gives
            <me>
                \abs{Kx(t)}^2 = \left(\int_a^b \abs{k(t,s)^2} \, ds\right) \left(\int_a^b \abs{x(s)}^2 \, ds\right).
            </me>
            Thus <m>K</m> is bounded and
            <me>
                \norm{K} \leq \left(\int_c^d \int_a^b \abs{k(t,s)}\, ds \, ds \right)^{1/2}.
            </me></p>
        </example>

        <example><title>the shift operator</title>
            <p>Consider the sequence space <m>\ell^2</m> and define the shift operator <m>S</m> on <m>\ell^2</m> by <m>S(x_1, x_2, \ldots) = (0, x_1, \ldots)</m>. Since <m>\norm{Sx} = \norm{x}</m> for all <m>x \in \ell^2</m>, we see that <m>S</m> is an isometry, which implies that <m>S</m> is bounded. The backwards shift <m>S\ad</m> is defined by <m>S(x_1, x_2, \ldots) = (x_2, x_3, \ldots)</m>. While <m>S\ad</m> is bounded and <m>\norm{S\ad} = 1</m>, it is immediate that <m>S\ad</m> is not an isometry.</p>
        </example>

        <p>As with linear functionals, continuity and boundedness are equivalent for linear operators on normed spaces.</p>

        <theorem xref="thm-continuous-linear-operator">
            <p>Let <m>V, W</m> be normed spaces, and let <m>T: V \to W</m> be a linear operator. The following are equivalent.
            <ol>
                <li><m>T</m> is bounded;</li>
                <li><m>T</m> is continuous;</li>
                <li><m>T</m> is continuous at <m>0</m>.</li>
            </ol></p>
        </theorem>

        <p>The proof of this theorem is identical to the proof of <xref ref="thm-continuous-linear-functional"/> with absolute values replaced by norms.</p>

        <p>In fact, many properties of linear functionals extend to the larger class of linear operators. For normed spaces <m>V, W</m>, we denote by <m>\L(V, W)</m> the set of bounded (and thus continuous) linear operators from <m>V \to W</m>.  Just as in the case of linear functionals, the bounded linear operators <m>\L(V, W)</m> form a vector space. If <m>V, W</m> are normed spaces, we can say more.
        </p>

        <theorem xref="thm-operators-banach-space">
            <p>Let <m>V, W</m> be normed spaces. Then <m>\L(V, W)</m> is a normed space with respect to the operator norm. Furthermore, if <m>W</m> is a Banach space, then so is <m>\L(V, W)</m>.</p>
        </theorem>

        <p>For compatible operators, we can work with compositions. We'll use the usual operator notation <m>BA</m> to mean <m>B \circ A</m> where <m>A: U \to V</m> and <m>B: V \to W</m>. So <m>BAx = B(Ax)</m>. The norms act in the manner you might suspect.</p>

        <theorem xml:id="thm-composition-norm">
            <p>Let <m>U, V, W</m> be vector spaces. If <m>A \in \L(U, V)</m> and <m>B \in \L(V, W)</m>, then <m>BA \in \L(U, W)</m> and 
            <me>
                \norm{AB} \leq \norm{A}\norm{B}.
            </me>
                </p>
        </theorem>

        <proof>
            <p>The operator <m>BA</m> inherits linearity from <m>A</m> and <m>B</m> and continuity as a composition of continuous functions. To see the inequality with the norms, for any <m>x \in U</m>, 
            <md>
                <mrow> \norm{BAx} \amp= \norm{B(Ax)} </mrow>
                <mrow> \amp \leq \norm{B}\norm{Ax}</mrow>
                <mrow> \amp \leq \norm{B}\norm{A}\norm{x}.</mrow>
            </md>
            We've shown that
            <me>
                \norm{BA} \leq \norm{B}\norm{A}.
            </me>
            </p>
        </proof>

        <p>In the case that <m>A \in \L(V)</m>, we denote repeated compositions of <m>A</m> in power notation. Hence <m>\underbrace{AA\cdots A}_{n \text{times}} = A^n</m>. As an immediate and useful consequence of <xref ref="thm-composition-norm"/>, 
        <me>
            \norm{A^n} \leq \norm{A}^n.
        </me></p>

    </section>

    <section>
        <title>Inverses</title>
        <p>Need to get this from notes. Big theorem here is</p>

        <theorem xml:id="thm-contraction-inverse">
            <p>Suppose that <m>A \in \L(V)</m>. If <m>\norm{A} \lt 1</m> then <m>I - A</m> is invertible and 
            <me>
                (I - A)\inv = \sum_{n=0}^\infty A^n,
            </me>
            where <m>A^0 = I_V</m>.</p>
        </theorem>

        <theorem xml:id="thm-invertible-open">
            <p>Let <m>E</m> be a Banach space. The set of invertible operators on <m>E</m> is an open subset of <m>\L(E)</m>.</p>
        </theorem>

    </section>

    <section>
        <title>Adjoint operators</title>
        <p>Restricting to our attention to <m>\C^n</m> for the moment, let <m>A</m> be an <m>m \times n</m> matrix (where we think of <m>A: \C^n \to \C^m</m>). Let <m>x \in \C^n</m> and <m>y \in \C^n</m>. Denote by <m>A\ad</m> the Hermitian transpose (or conjugate transpose) of <m>A</m> (which if course acts in the other direction <m>A: \C^m \to \C^n</m>). Then in the inner product, we get the relationship
        <md>
            <mrow> \ip{Ax}{y} \amp= y\ad (Ax)</mrow>
            <mrow> \amp = (y\ad A) x </mrow>
            <mrow> \amp= (A\ad y)\ad x</mrow>
            <mrow> \amp=\ip{x}{A\ad y}.</mrow>
        </md>
        We seek to define a similar notion of the adjoint operator in the infinite dimensional case by way of the Riesz representation theorem.</p>

        <theorem xml:id="thm-adjoint">
            <p>Let <m>A \in \L(V, W)</m> where <m>V, W</m> are Hilbert spaces. There is a unique operator <m>A\ad \in \L(W, V)</m> such that <me>\ip{Ax}{y}_W = \ip{x}{A\ad y}_V</me> for all <m>x \in V, y \in W</m>.</p>
        </theorem>

        <proof>
            <p> For any <m>y \in W</m>, the map
                <me>
                    x \mapsto \ip{Ax}{y}_W
                </me>
                is a continuous linear functional on <m>V</m>. Then <xref ref="thm-riesz-rep"/> gives the existence of a unique vector <m>z \in V</m> so that
                <me>
                    \ip{Ax}{y}_W = \ip{x}{z}_V
                </me>
                for all <m>x \in V</m>. Then we define the map <m>A\ad: W \to V</m> by
                <me>
                    A\ad y = z.
                </me>
                It remains to show that <m>A\ad</m> is a continuous linear functional.</p>

                <p>To see that <m>A\ad</m> is linear, choose <m>\alpha, \beta \in \C</m> and <m>u, v \in W</m>. Then for all <m>x \in V</m>, we have 
                <md>
                    <mrow>\ip{x} {A\ad(\alpha u + \beta v)} \amp = \ip{Ax}{\alpha u + \beta v}</mrow>
                    <mrow> \amp= \cc\alpha \ip{Ax}{u} + \cc\beta\ip{Ax}{v}</mrow>
                    <mrow> \amp = \cc\alpha \ip{x}{A\ad u} + \cc\beta\ip{x}{A\ad v}</mrow>
                    <mrow> \amp= \ip{x}{ \alpha A\ad u + \beta A\ad v}.</mrow>
                </md>
                Since this holds for all <m>x \in V</m>, we must have 
                <me>
                    A\ad(\alpha u + \beta v) =  \alpha A\ad u + \beta A\ad v.
                </me></p>

                <p>To see that <m>A\ad</m> is bounded, notice that for all <m>y \in W,</m>
                <md>
                    <mrow> \norm{A\ad y}^2 \amp= \ip{A\ad y}{A\ad y} </mrow>
                    <mrow> \amp=\ip{A A\ad y}{y}</mrow>
                    <mrow> \amp\leq \norm{A A\ad y}\norm{y}</mrow>
                    <mrow> \amp\leq \norm{A} \norm{A\ad y}\norm{y}.</mrow>
                </md>
                Dividing through, we get
                <me>
                    \norm{A\ad y} \leq \norm{A} \norm{y}
                </me>
                which gives that <m>\norm{A\ad} \leq \norm{A}</m>, and so <m>A\ad</m> is bounded.</p>

                <p>Uniqueness follows from the observation that if <m>A</m> had two adjoints, then for all <m>x \in V</m>, we'd have
                <me>
                    \ip{x}{A_1\ad y} = \ip{x}{A_2\ad y},
                </me>
                and so <m>A_1\ad = A_2\ad</m>.</p>
        </proof>

        <p>In matrices, <m>(A\ad)\ad = A</m>. The same is true of operators on Hilbert spaces.</p>

        <theorem xml:id="thm-double-adjoint">
            <p>Suppose that <m>V, W</m> are Hilbert spaces and <m>A \in \L(V, W)</m>. Then <m>A^{**} = A</m> and <m>\norm{A\ad} = \norm{A}</m>.</p>
        </theorem>

        <proof>
            <p>Suppose that <m>A \in \L(V, W), B = A\ad \in \L(W, V)</m>, and <m>B\ad \in \L(V, W)</m>. We claim that <m>B\ad = A</m>. For all <m>x \in V, y \in W</m>, we have
            <md>
                <mrow>\ip{y}{B\ad x} \amp= \ip{By}{x}</mrow>
                <mrow>\amp= \ip{A\ad y}{x}</mrow>
                <mrow>\amp= \cc{\ip{x}{A\ad y}}</mrow>
                <mrow>\amp= \cc{\ip{Ax}{y}}</mrow>
                <mrow>\amp= \ip{y}{Ax}.</mrow>
            </md>
            Then <m>B\ad x = Ax</m> for all <m>x\in V</m> and so <m>B\ad = A</m>.
            </p>

            <p>To see that <m>\norm{A\ad} = \norm{A}</m>, recall that the proof of the previous theorem established that <m>\norm{A\ad} \leq \norm{A}</m>. Applying this  to <m>A^{**}</m> gives that <m>\norm{A^{**}}\leq \norm{A\ad}</m> and hence the result.</p>
        </proof>
    </section>

    <section>
        <title>Hermitian operators</title>
        <p>A self-adjoint or Hermitian matrix is a complex <m>n\times n</m> matrix satisfying <m>A = A\ad</m>. Such matrices have a wealth of structural results available to describe them. For example, such matrices have real eigenvalues, can be unitarily diagonalized, and carry a partial order (called the <url href="https://en.wikipedia.org/wiki/Loewner_order">Loewner order</url>) with a notion of positivity. We're interested in developing analogous results in the infinite dimensional setting.
        </p>

        <definition xml:id="def-hermitian-operator">
            <p>Let <m>\hilbert</m> be a Hilbert space and <m>A \in \L(\hilbert)</m>. We say that <m>A</m> is <term>Hermitian</term> (or self-adjoint) if <m>A = A\ad</m>.</p>
        </definition>

        <p>Hermitian operators have a nice expression for calculating norms.</p>

        <theorem xml:id="thm-hermitian-norm">
            <p>If <m>A</m> is a Hermitian operator on a Hilbert space <m>\hilbert</m>, then 
            <me>
                \norm{A} = \sup_{\norm{x} = 1} \abs{\ip{Ax}{x}}.
            </me></p>
        </theorem>

        <proof>
            <p>As an immediate consequence of <xref ref="thm-cauchy-schwarz"/>, for any <m>\norm{x} = 1</m>, we have
                <me>
                    \abs{\ip{Ax}{x}}\leq \norm{A}\norm{x}^2 = \norm{A}.
                </me>
            </p>

            <p>In the other direction we have a bit more work to do. Let <m>M = \sup_{\norm{x} = 1} \abs{\ip{Ax}{x}}</m>. Note that for all <m>u</m>, we have <m>\abs{\ip{Au}{u}} \leq M\norm{u}^2</m> (which one should check!). Now assume that <m>\norm{x} = 1</m> and <m>\norm{y} = 1</m>. Using that <m>A\ad = A</m>),
                <me>
                    \ip{A(x\pm y)}{x\pm y} = \ip{Ax}{x}\pm 2 \RE \ip{Ax}{y} + \ip{Ay}{y}.
                </me>
            If we subtract one of these equations from the other, we get
                <md> 
                    <mrow>4\RE\ip{Ax}{y} \amp= \ip{A(x+y)}{x+y} - \ip{A(x-y)}{x-y}</mrow>
                    <mrow>\amp \leq M(\norm{x + y}^2 + \norm{x-y}^2)</mrow>
                </md>
            Now apply <xref ref="thm-parallelogram"/> to get
                <me>
                    4 \RE \ip{Ax}{y} \leq 2M(\norm{x}^2 + \norm{y}^2) = 4M.
                </me>
            Suppose that <m>\ip{Ax}{y} = e^{it} \abs{\ip{Ax}{y}}</m>. Then replacing <m>x</m> with <m>e^{-it} x</m> in the inequality gives 
            <me>
                \abs{\ip{Ax}{y}} \leq M
            </me>
            when <m>\norm{x} = \norm{y} = 1</m>. Notice that when <m>y = Ax/\norm{Ax}</m>, we get
            <me>
                \norm{Ax}^2 \leq M \norm{Ax}
            </me>
            and so 
            <me>
                \norm{Ax} \leq M
            </me>
            for all <m>\norm{x} = 1</m>. But then <m>\norm{A} \leq M = \sup_{\norm{x} = 1} \abs{\ip{Ax}{x}}</m>.</p>
        </proof>
    </section>

    <section>
        <title>The spectrum</title>
        <p>If <m>A</m> is an <m>n \times n</m> complex matrix, then <m>A</m> has at least one eigenvalue. That is, there exists <m>\la \in \C</m> such that <m>Av = \la v</m> for some non-zero vector <m>v \in \C^n</m>. The spectrum of a matrix, denoted <m>\sigma(A)</m> is the set consisting of its eigenvalues. The spectrum of a matrix is used to study its structure and behavior as a linear map. We'd like to extend the notion of spectrum to the infinite dimensional setting, so we'll take a moment to look at how we find it for matrices. In elementary linear algebra, we learn that the eigenvalues of <m>A</m> are the solutions to the equation <m>\det(A - \la I) = 0</m>. Of course what we're actually measuring with this equation is the (lack of) invertibility of the operator <m>A - \la I</m>, which we could also encode geometrically with the condition that <m>\ker (A - \la I) \neq \{0\}</m>. Since determinants aren't all that useful or easy to define in the infinite dimensional setting, we'll use these to define the spectum of an operator.</p>

        <definition xml:id="def-spectrum">
            <p>Let <m>E</m> be a Banach space and <m>A \in \L(E)</m>. The <term>spectrum</term> of <m>A</m>, denoted <m>\sigma(A)</m> is the set 
                <me>
                    \sigma(A) = \{\la \in \C : \la I - A \text{ is not invertible.}\}
                </me></p>
        </definition>

        <p>Given <m>A \in \L(E)</m>, we have that <m>\sigma(A)</m> is non-empty. (Else, the map <m>\la \mapsto (\la I - A)\inv</m> would be a bounded, non-constant, entire <m>\L(E)</m>-valued map, which would violate a version of <url href="https://en.wikipedia.org/wiki/Liouville%27s_theorem_(complex_analysis)">Liouville's theorem</url>). If <m>E</m> is finite dimensional, then <m>\sigma(A)</m> consists of the eigenvalues of <m>A</m>. We can still use the term <emph>eigenvalue</emph> in the infinite dimensional setting in the exact same way as the finite case. Notice though that operators need not possess eigenvalues. For example, the shift operator <m>S</m> acting on <m>\ell^2</m> has no eigenvalues. (Exercise). On the other hand, since <m>S</m> isn't invertible, <m>0 \in \sigma(A)</m>. This illustrates the significantly more complex situation that occurs in infinite dimensions. Notice that if <m>\la</m> is an eigenvalue of <m>A</m>, then <m>\la \in \sigma(A)</m>.</p>

        <p>The proof of the following result, analogous to the finite case, is left as an easy exercise.</p>

        <proposition>
            <p>If <m>A \in \L(E)</m> is a Hermitian operator on a Hilbert space <m>\hilbert</m>, and <m>\la</m> is an eigenvalue of <m>A</m>, then <m>\la \in \R</m>.</p>
        </proposition>

        <p>A useful topological property of the spectrum is that it forms a compact subset of <m>\C</m>. (In the finite dimensional case, this is apparent.)</p>

        <theorem xml:id="thm-compact-spectrum">
            <p>Let <m>E</m> be a Banach space and <m>A \in \L(E)</m>. Then <m>\sigma(A)</m> is a compact subset of <m>\C</m> and 
            <me>
                \sigma(A) \subseteq \{z \in \C: \abs{z} \leq \norm{A}\}.
            </me></p>
        </theorem>

        <proof>
            <p>Define a function <m>F: \C \to \L(E)</m> by <m>F(\la) = \la I - A</m>. For any <m>\la, \mu \in \C</m>, 
            <md>
                <mrow>\norm{F(\la) - F(\mu)} \amp= \norm{(\la I - A) - (\mu I - A)}</mrow>
                <mrow>\amp = \abs{\la - \mu}</mrow>
            </md>
            and so <m>F</m> is Lipschitz and therefore continuous in the operator norm. Now let <m>GL(E)</m> denote the invertible elements of <m>E</m>, which is an open subset of <m>\L(E)</m> by <xref ref="thm-invertible-open"/>. Then <m>\L(E)-GL(E)</m> is closed, and by continuity so is <m>F\inv(\L(E)-GL(E)) = \sigma(A)</m>.</p>

            <p>Now let <m>\la \in \C</m> with <m>\abs{\la} \gt \norm{A}</m>. Then <m>\norm{\la\inv A} \lt 1</m>, and so <m>I - \la\inv A</m> is invertible by <xref ref="thm-contraction-inverse"/>. But then <m>\la\inv(\la I - A)</m> is invertible and so must be <m>\la I - A</m>, and so <m>\la \notin \sigma(A)</m>. Then <m>\sigma(A)</m> is in the closed disk of radius <m>\norm{A}</m> and hence is bounded. As closed and bounded subsets of <m>\C</m> are compact, we have established the result. </p>
        </proof>

    </section>
</chapter>
